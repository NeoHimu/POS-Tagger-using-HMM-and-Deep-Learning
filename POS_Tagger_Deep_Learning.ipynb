{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parts of Speech Tagger using Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In my previous trial, I build a POS tagger that uses Hidden Markov Model based probabilistic model. Here, I will be building POS tagger that uses deep learning models e.g. LSTM, Bi-LSTM,GRU. I will break down whole project into steps so that you can understand it better and I can debug better. I can think of dividing it into four steps : 1.) Get word embeddings  2.) Prepare data for training 3.) Train and 4.) Test (Evaluate the result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1 : I will be using Glove word embedding developed at Stanford. One can use any word embedding like word2vec developed by Google. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 : Dealing with Word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the glove.6B.100d.txt file from https://nlp.stanford.edu/projects/glove/ . This file consists of word separated by it's embedding. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedding = {} #This is a dictionary whose key will be word and value will be it's embedding\n",
    "word_embedding_file = open('glove.6B/glove.6B.100d.txt', encoding=\"utf8\") # This just opend the file 'glove.6B.100d.txt'\n",
    "\n",
    "for line in word_embedding_file:\n",
    "    temp = line.split() # Convert each line into a list of elements. First ele is word and rest is embedding. \n",
    "    word = temp[0]                                    # Word is copied\n",
    "    embedding = np.asarray(temp[1:], dtype='float32') # Embedding is copied\n",
    "    word_embedding[word] = embedding # word is made the key of the dictionary and embedding is made the value of the dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After opening the file, iterate through each line in it. Note that each line is in the form of a word separated by it's embedding by space e.g. play 12.9 10.5 11.8 192.8 ... \n",
    "So, line.split() just converts above example into a list \\[ play, 12.9, 10.5, 11.8, 192.8, ...\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('PickledWordEmbeddings/'):\n",
    "    os.makedirs('PickledWordEmbeddings/')\n",
    "    print(\"Does not exist.. so making the directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if PickledWordEmbeddings directory exists or not. If PickledWordEmbeddings does not exist make a directory with this name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('PickledWordEmbeddings/Glove.pkl', 'wb') as f:\n",
    "    pickle.dump(word_embedding, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the Glove.pkl file (if this file does not exist then open() funtion will create a file with this name) in PickledWordEmbeddings/ directory and dump the dictionary (word_embedding) we created in this file. Note that we'll have to fetch these embeddings in a dictionary only when we load the Glove.pkl file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2 : Now we are done with saving the word embeddings on the disk, we move on to prepare the data required for training the neural network model. We'll get the data from the famous Brown Corpus. We can get this data from nltk library from Stanford (https://www.nltk.org/data.html : visit this link to know more about downloading)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brown Corpus preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_files = os.listdir('brown/') # lists names of all files present in the brown/ folder.\n",
    "\n",
    "temp_corpus = '' # Contains all lines from all files separated by \\n\n",
    "\n",
    "for file in list_of_files[0:100]:  # There are around 500 files but I am just using 100 for the sake of making it small in scale\n",
    "    with open('brown/' + file) as f:\n",
    "        temp_corpus = temp_corpus + '\\n' + f.read() # Here actual reading of content of file occured\n",
    "\n",
    "corpus = temp_corpus.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sentences_words = [] # each element is a list of words present in a single sentence \n",
    "sentences_tags = [] # each element is a list of tags present in a single sentence\n",
    "\n",
    "#We need words in each line separately as we are going to feed a sentence in the neural network not an individual word\n",
    "for line in corpus:\n",
    "    if(len(line)>0):\n",
    "        words_in_a_line = [] #temporary storage for words in a line\n",
    "        tags_in_a_line = [] #temporary storage for tags in a line\n",
    "        for word in line.split():\n",
    "            try:            \n",
    "                w, tag = word.split('/')\n",
    "            except: # If the w/tag form is not present by mistake\n",
    "                break\n",
    "\n",
    "            words_in_a_line.append(w.lower())\n",
    "            tags_in_a_line.append(tag)\n",
    "        \n",
    "        sentences_words.append(words_in_a_line) \n",
    "        sentences_tags.append(tags_in_a_line)\n",
    "\n",
    "# print(sentences_words[0])\n",
    "# print(sentences_tags[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved as pickle file\n"
     ]
    }
   ],
   "source": [
    "#including words from the test file into voacb\n",
    "list_of_files = os.listdir('test/') # lists names of all files present in the brown/ folder.\n",
    "\n",
    "temp_corpus = '' # Contains all lines from all files separated by \\n\n",
    "\n",
    "for file in list_of_files:  # There are around 500 files but I am just using 100 for the sake of making it small in scale\n",
    "    with open('test/' + file) as f:\n",
    "        temp_corpus = temp_corpus + '\\n' + f.read() # Here actual reading of content of file occured\n",
    "\n",
    "corpus = temp_corpus.split('\\n')\n",
    "\n",
    "sentences_words_temp = sentences_words # each element is a list of words present in a single sentence \n",
    "sentences_tags_temp = sentences_tags # each element is a list of tags present in a single sentence\n",
    "\n",
    "#We need words in each line separately as we are going to feed a sentence in the neural network not an individual word\n",
    "for line in corpus:\n",
    "    if(len(line)>0):\n",
    "        words_in_a_line = [] #temporary storage for words in a line\n",
    "        tags_in_a_line = [] #temporary storage for tags in a line\n",
    "        for word in line.split():\n",
    "            try:            \n",
    "                w, tag = word.split('/')\n",
    "            except: # If the w/tag form is not present by mistake\n",
    "                break\n",
    "\n",
    "            words_in_a_line.append(w.lower())\n",
    "            tags_in_a_line.append(tag)\n",
    "        \n",
    "        sentences_words_temp.append(words_in_a_line) \n",
    "        sentences_tags_temp.append(tags_in_a_line)\n",
    "\n",
    "# print(sentences_words[0])\n",
    "# print(sentences_tags[0])\n",
    "\n",
    "\n",
    "vocab_words = set(sum(sentences_words_temp, [])) #flattening of list is being done followed by finding unique words\n",
    "vocab_tags = set(sum(sentences_tags_temp, [])) # This gives total number of tags \n",
    "\n",
    "# assert len(X_train) == len(Y_train)\n",
    "\n",
    "word2int = {}\n",
    "int2word = {}\n",
    "\n",
    "word2int = dict((w, i) for i, w in enumerate(vocab_words))\n",
    "int2word = dict((i, w) for i, w in enumerate(vocab_words))\n",
    "\n",
    "tag2int = {}\n",
    "int2tag = {}\n",
    "\n",
    "tag2int = dict((t, i) for i, t in enumerate(vocab_tags))\n",
    "int2tag = dict((i, t) for i, t in enumerate(vocab_tags))\n",
    "\n",
    "sentences_words_num = [[word2int[word] for word in sentence] for sentence in sentences_words]\n",
    "sentences_tags_num = [[tag2int[word] for word in sentence] for sentence in sentences_tags]\n",
    "\n",
    "# print('sample X_train_numberised: ', sentences_words_num[0], '\\n')\n",
    "# print('sample Y_train_numberised: ', sentences_tags_num[0], '\\n')\n",
    "\n",
    "X_train_numberised = np.asarray(sentences_words_num)\n",
    "Y_train_numberised = np.asarray(sentences_tags_num)\n",
    "\n",
    "pickle_files = [sentences_words_num, sentences_tags_num, word2int, int2word, tag2int, int2tag]\n",
    "\n",
    "if not os.path.exists('PickledData/'):\n",
    "    print('PickledData/ is created to save pickled glove file')\n",
    "    os.makedirs('PickledData/')\n",
    "\n",
    "with open('PickledData/data.pkl', 'wb') as f:\n",
    "    pickle.dump(pickle_files, f)\n",
    "\n",
    "print('Saved as pickle file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "294\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 100, 100)          2029700   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 100, 128)          84480     \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 100, 295)          38055     \n",
      "=================================================================\n",
      "Total params: 2,152,235\n",
      "Trainable params: 2,152,235\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/2\n",
      " - 95s - loss: 0.5567 - acc: 0.8934 - val_loss: 0.2039 - val_acc: 0.9569\n",
      "Epoch 2/2\n",
      " - 88s - loss: 0.1301 - acc: 0.9708 - val_loss: 0.0926 - val_acc: 0.9770\n",
      "MODEL SAVED in Models/ as model.h5\n",
      "TEST LOSS 0.088579 \n",
      "TEST ACCURACY: 0.977693\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Input\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import LSTM, Bidirectional\n",
    "from keras.models import Model\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# PARAMETERS ================\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "EMBEDDING_DIM = 100\n",
    "TEST_SPLIT = 0.2\n",
    "VALIDATION_SPLIT = 0.2\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "\n",
    "with open('PickledData/data.pkl', 'rb') as f:\n",
    "    X, y, word2int, int2word, tag2int, int2tag = pickle.load(f)\n",
    "\n",
    "\n",
    "def generator(all_X, all_y, n_classes, batch_size=BATCH_SIZE):\n",
    "    num_samples = len(all_X)\n",
    "\n",
    "    while True:\n",
    "\n",
    "        for offset in range(0, num_samples, batch_size):\n",
    "            \n",
    "            X = all_X[offset:offset+batch_size]\n",
    "            y = all_y[offset:offset+batch_size]\n",
    "\n",
    "            y = to_categorical(y, num_classes=n_classes)\n",
    "\n",
    "\n",
    "            yield shuffle(X, y)\n",
    "\n",
    "\n",
    "n_tags = len(tag2int)\n",
    "print(n_tags)\n",
    "X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "y = pad_sequences(y, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "# y = to_categorical(y, num_classes=len(tag2int) + 1)\n",
    "\n",
    "# print('TOTAL TAGS', len(tag2int))\n",
    "# print('TOTAL WORDS', len(word2int))\n",
    "\n",
    "# shuffle the data\n",
    "X, y = shuffle(X, y)\n",
    "\n",
    "# split data into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SPLIT,random_state=42)\n",
    "\n",
    "# split training data into train and validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=VALIDATION_SPLIT, random_state=1)\n",
    "\n",
    "n_train_samples = X_train.shape[0]\n",
    "n_val_samples = X_val.shape[0]\n",
    "n_test_samples = X_test.shape[0]\n",
    "\n",
    "# print('We have %d TRAINING samples' % n_train_samples)\n",
    "# print('We have %d VALIDATION samples' % n_val_samples)\n",
    "# print('We have %d TEST samples' % n_test_samples)\n",
    "\n",
    "# make generators for training and validation\n",
    "train_generator = generator(all_X=X_train, all_y=y_train, n_classes=n_tags + 1)\n",
    "validation_generator = generator(all_X=X_val, all_y=y_val, n_classes=n_tags + 1)\n",
    "\n",
    "\n",
    "\n",
    "with open('PickledWordEmbeddings/Glove.pkl', 'rb') as f:\n",
    "\tembeddings_index = pickle.load(f)\n",
    "\n",
    "# print('Total %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "# + 1 to include the unkown word\n",
    "embedding_matrix = np.random.random((len(word2int) + 1, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in word2int.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embeddings_index will remain unchanged and thus will be random.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# print('Embedding matrix shape', embedding_matrix.shape)\n",
    "# print('X_train shape', X_train.shape)\n",
    "\n",
    "embedding_layer = Embedding(len(word2int) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=True)\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "l_lstm = Bidirectional(LSTM(64, return_sequences=True))(embedded_sequences)\n",
    "preds = TimeDistributed(Dense(n_tags + 1, activation='softmax'))(l_lstm)\n",
    "model = Model(sequence_input, preds)\n",
    "\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "\n",
    "# print(\"model fitting - Bidirectional LSTM\")\n",
    "model.summary()\n",
    "\n",
    "model.fit_generator(train_generator, \n",
    "                     steps_per_epoch=n_train_samples//BATCH_SIZE,\n",
    "                     validation_data=validation_generator,\n",
    "                     validation_steps=n_val_samples//BATCH_SIZE,\n",
    "                     epochs=2,\n",
    "                     verbose=2,\n",
    "                     workers=4)\n",
    "\n",
    "if not os.path.exists('Models/'):\n",
    "    print('MAKING DIRECTORY Models/ to save model file')\n",
    "    os.makedirs('Models/')\n",
    "\n",
    "train = True\n",
    "\n",
    "if train:\n",
    "    model.save('Models/model.h5')\n",
    "    print('MODEL SAVED in Models/ as model.h5')\n",
    "else:\n",
    "    from keras.models import load_model\n",
    "    model = load_model('Models/model.h5')\n",
    "\n",
    "y_test = to_categorical(y_test, num_classes=n_tags+1)\n",
    "test_results = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('TEST LOSS %f \\nTEST ACCURACY: %f' % (test_results[0], test_results[1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 Most frequent tags!\n",
      "['nn', 'in', 'at', 'jj', 'vb', 'ppss', 'nns', 'ppo', 'rb', 'vbd']\n",
      "['at', 'in', 'jj', 'nn', 'nns', 'ppo', 'ppss', 'rb', 'vb', 'vbd', 'vbn']\n",
      "[[26  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 28  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0 11  2  0  0  0  1  0  0  1]\n",
      " [ 0  0  0 32  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  1 11  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 10  1  0  0  0  0]\n",
      " [ 0  0  0  0  0  0 14  0  0  0  0]\n",
      " [ 0  0  0  1  0  0  0  9  0  0  0]\n",
      " [ 0  0  1  0  0  0  0  0 13  0  0]\n",
      " [ 0  0  0  0  0  0  0  1  0  8  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "\n",
    "with open('PickledData/data.pkl', 'rb') as f:\n",
    "    X_train, Y_train, word2int, int2word, tag2int, int2tag = pickle.load(f)\n",
    "\n",
    "    del X_train\n",
    "    del Y_train\n",
    "\n",
    "\n",
    "with open('test_pickle_file/test_data.pkl', 'rb') as f:\n",
    "    test_sentence, y_actual_tag, frequent_pos = pickle.load(f)\n",
    "\n",
    "y_pred_tag  = []  \n",
    "    \n",
    "for sentence in test_sentence:\n",
    "#     sentence = 'he is a good boy'.split()\n",
    "\n",
    "    tokenized_sentence = []\n",
    "\n",
    "    for word in sentence:\n",
    "        tokenized_sentence.append(word2int[word])\n",
    "\n",
    "    tokenized_sentence = np.asarray([tokenized_sentence])\n",
    "    padded_tokenized_sentence = pad_sequences(tokenized_sentence, maxlen=100)\n",
    "\n",
    "    # print('The sentence is ', sentence)\n",
    "    # print('The tokenized sentence is ',tokenized_sentence)\n",
    "    # print('The padded tokenized sentence is ', padded_tokenized_sentence)\n",
    "\n",
    "    model = load_model('Models/model.h5')\n",
    "\n",
    "    prediction = model.predict(padded_tokenized_sentence)\n",
    "\n",
    "#     print(prediction.shape)\n",
    "    out = []\n",
    "    for i in range(100-len(sentence),100):\n",
    "        out.append(int2tag[np.argmax(prediction[0][i])])\n",
    "        \n",
    "    y_pred_tag.append(out)\n",
    "    \n",
    "y_pred_tag = sum(y_pred_tag,[]) \n",
    "\n",
    "\n",
    "actual = []\n",
    "predicted = []\n",
    "\n",
    "print(\"10 Most frequent tags!\")\n",
    "print(frequent_pos)\n",
    "\n",
    "for idx2, pos in enumerate(y_actual_tag):\n",
    "    if pos in frequent_pos:\n",
    "        actual.append(pos)\n",
    "        predicted.append(y_pred_tag[idx2])\n",
    "\n",
    "temp_labels = sorted(list(set(actual+predicted)))\n",
    "print(temp_labels)\n",
    "cm = confusion_matrix(actual, predicted, temp_labels)\n",
    "print(cm)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
